{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # Correct class from HuggingFace\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core import Settings, VectorStoreIndex, PromptTemplate, SimpleDirectoryReader\n",
    "from llama_index.llms.ollama import Ollama  # Assuming you corrected the import here\n",
    "\n",
    "# Loading documents from directory\n",
    "\n",
    "import os\n",
    "from elasticsearch import Elasticsearch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llama_index.core import VectorStoreIndex, PromptTemplate, SimpleDirectoryReader\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the embedding model\n",
    "def load_embedding_model(model_name=\"sentence-transformers/all-mpnet-base-v2\", device=\"cuda\"):\n",
    "    model_kwargs = {\"device\": device}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    return HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-besqHNvavvOogW1gXihmT3BlbkFJwLDqlxM3BE7iX1P9D40o\"\n",
    "\n",
    "\n",
    "# LLM (gpt-3.5)\n",
    "gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# LLM (gpt-4)\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "\n",
    "# gpt-4\n",
    "step_decompose_transform = StepDecomposeQueryTransform(llm=gpt4, verbose=True)\n",
    "\n",
    "# gpt-3\n",
    "step_decompose_transform_gpt3 = StepDecomposeQueryTransform(\n",
    "    llm=gpt35, verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# Load documents from a directory and index them in Elasticsearch and VectorStoreIndex\n",
    "def load_and_index_documents(directory):\n",
    "    reader = SimpleDirectoryReader(directory, recursive=True)\n",
    "    docs = reader.load_data()\n",
    "    # Example to check the structure of docs\n",
    "    if docs and isinstance(docs[0], dict) and 'id' in docs[0] and 'content' in docs[0]:\n",
    "        index_documents(docs)\n",
    "    else:\n",
    "        logger.error(\"Document structure not as expected.\")\n",
    "    return docs\n",
    "\n",
    "docs = load_and_index_documents(\"/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai\")\n",
    "lc_embedding_model = load_embedding_model()\n",
    "embed_model = LangchainEmbedding(lc_embedding_model)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(docs)\n",
    "index.embedding_model = embed_model\n",
    "\n",
    "# set Logging to DEBUG for more detailed outputs\n",
    "from llama_index.core.query_engine import MultiStepQueryEngine\n",
    "\n",
    "query_engine = index.as_query_engine(llm=gpt35)\n",
    "query_engine = MultiStepQueryEngine(\n",
    "    query_engine=query_engine,\n",
    "    query_transform=step_decompose_transform_gpt3,\n",
    ")\n",
    "response_gpt35 = query_engine.query(\n",
    "  \"What all services are there in the project and what is the purpose of each service aned what does vendor service do?\",\n",
    ")\n",
    "\n",
    "print(str(response_gpt35))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
