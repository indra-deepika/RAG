{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import faiss\n",
    "# from elasticsearch import Elasticsearch\n",
    "# from elasticsearch_dsl import Search, Q\n",
    "\n",
    "# # Assuming documents are preprocessed into a list of dicts with 'id', 'text', and 'embedding'\n",
    "# documents = [\n",
    "#     {'id': '1', 'text': 'example text', 'embedding': np.random.rand(768).astype('float32')}\n",
    "# ]\n",
    "\n",
    "# # Set up Vector Index with Faiss\n",
    "\n",
    "# dimension = 768  # Dimension of embeddings\n",
    "# vector_index = faiss.IndexFlatL2(dimension)\n",
    "# vector_index.add(np.array([doc['embedding'] for doc in documents]))\n",
    "\n",
    "# # Set up Elasticsearch for BM25\n",
    "# es = Elasticsearch(\"http://localhost:9200\")\n",
    "# # Assuming Elasticsearch index 'documents' is set up with mappings appropriate for text\n",
    "\n",
    "# for doc in documents:\n",
    "#     es.index(index='documents', id=doc['id'], body={'text': doc['text']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search(query_embedding, query_text, top_k=10):\n",
    "#     # Vector search with Faiss\n",
    "#     distances, indices = vector_index.search(np.array([query_embedding]), top_k)\n",
    "    \n",
    "#     # Text search with Elasticsearch\n",
    "#     s = Search(using=es, index='documents').query(\"match\", text=query_text)\n",
    "#     response = s.execute()\n",
    "\n",
    "#     return indices[0], response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reciprocal_rank_fusion(vector_results, text_results):\n",
    "#     rank_scores = {}\n",
    "#     for rank, idx in enumerate(vector_results):\n",
    "#         doc_id = documents[idx]['id']\n",
    "#         rank_scores[doc_id] = rank_scores.get(doc_id, 0) + 1 / (rank + 1)\n",
    "    \n",
    "#     for rank, hit in enumerate(text_results):\n",
    "#         doc_id = hit.meta.id\n",
    "#         rank_scores[doc_id] = rank_scores.get(doc_id, 0) + 1 / (rank + 1)\n",
    "    \n",
    "#     # Sort by combined rank score\n",
    "#     sorted_docs = sorted(rank_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "#     return [doc_id for doc_id, _ in sorted_docs[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import faiss\n",
    "import sqlite3\n",
    "from transformers import pipeline, DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "# Setup directory and transformer model for embedding generation\n",
    "transcript_directory =  \"/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "feature_extractor = pipeline('feature-extraction', model='distilbert-base-cased')\n",
    "\n",
    "# Define a Document class with necessary methods\n",
    "class Document:\n",
    "    def __init__(self, id_, text, embedding):\n",
    "        self.id_ = id_\n",
    "        self.text = text\n",
    "        self.embedding = embedding\n",
    "\n",
    "    def get_doc_id(self):\n",
    "        return self.id_\n",
    "\n",
    "    def hash(self):\n",
    "        return hash((self.id_, self.text))  # Example hash function based on id and text\n",
    "\n",
    "# Function to read and process files\n",
    "def process_files(directory):\n",
    "\n",
    "    reader = SimpleDirectoryReader(directory, filename_as_id=True, recursive=True )\n",
    "    docs = reader.load_data()\n",
    "    for doc in docs:\n",
    "        filename = doc.id_\n",
    "        content = doc.text\n",
    "        inputs = tokenizer(content, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = feature_extractor.model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        # doc.embedding = embedding.astype('float32')\n",
    "        document = Document(filename, content[:1024], embedding.astype('float32'))\n",
    "        documents.append(document)\n",
    "    return documents , docs\n",
    "\n",
    "# Load and process documents\n",
    "documents , docs = process_files(transcript_directory)\n",
    "\n",
    "# Set up Vector Index with Faiss\n",
    "dimension = 768  # Dimension of embeddings\n",
    "vector_index = faiss.IndexFlatL2(dimension)\n",
    "vector_index.add(np.array([doc.embedding for doc in documents]))\n",
    "\n",
    "# Set up SQLite Database with Full Text Search (FTS5)\n",
    "conn = sqlite3.connect('example.db')\n",
    "c = conn.cursor()\n",
    "c.execute('''\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS documents USING fts5(id, text)\n",
    "''')\n",
    "for doc in documents:\n",
    "    c.execute(\"INSERT INTO documents (id, text) VALUES (?, ?)\", (doc.get_doc_id(), doc.text))\n",
    "conn.commit()\n",
    "\n",
    "def search(query_embedding, query_text, top_k=10):\n",
    "    # Vector search with Faiss\n",
    "    _, indices = vector_index.search(np.array([query_embedding]), top_k)\n",
    "    \n",
    "    # Text search with SQLite FTS\n",
    "    query = f\"SELECT id FROM documents WHERE documents MATCH '{query_text}'\"\n",
    "    c.execute(query)\n",
    "    text_results = c.fetchall()\n",
    "\n",
    "    return indices[0], [res[0] for res in text_results]\n",
    "\n",
    "def reciprocal_rank_fusion(vector_results, text_results, top_k=10):\n",
    "    rank_scores = {}\n",
    "    for rank, idx in enumerate(vector_results):\n",
    "        doc_id = documents[idx].get_doc_id()\n",
    "        rank_scores[doc_id] = rank_scores.get(doc_id, 0) + 1 / (rank + 1)\n",
    "    \n",
    "    for rank, doc_id in enumerate(text_results):\n",
    "        rank_scores[doc_id] = rank_scores.get(doc_id, 0) + 1 / (rank + 1)\n",
    "    \n",
    "    sorted_docs = sorted(rank_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    # return [doc_id for doc_id, _ in sorted_docs[:top_k]]\n",
    "    return sorted_docs[:top_k]\n",
    "\n",
    "# Example usage\n",
    "\n",
    "query_text = \"What does vendor service do\"\n",
    "query_embedding = np.mean(feature_extractor(query_text), axis=1)[0]\n",
    "vector_results, text_results = search(query_embedding, query_text, top_k=5)\n",
    "final_results = reciprocal_rank_fusion(vector_results, text_results, top_k=5)\n",
    "print(\"Final combined results:\", final_results)\n",
    "\n",
    "# Close the SQLite connection\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, PromptTemplate\n",
    "from llama_index.llms.ollama import Ollama  \n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # Correct class from HuggingFace\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core import Settings, VectorStoreIndex, PromptTemplate, SimpleDirectoryReader\n",
    "from llama_index.llms.ollama import Ollama  # Assuming you corrected the import here\n",
    "\n",
    "# find docs based on doc_id\n",
    "def find_doc(doc_id, docs):\n",
    "    for doc in docs:\n",
    "        if doc.id_ == doc_id:\n",
    "            return doc\n",
    "    return None\n",
    "\n",
    "# find corresponding docs of sorted doc_ids\n",
    "def find_docs(doc_ids, docs):\n",
    "    return [find_doc(doc_id, docs) for doc_id in doc_ids]\n",
    "\n",
    "\n",
    "relevant_docs = find_docs([doc_id for doc_id, _ in final_results], docs)\n",
    "\n",
    "print(\"relevant_docs\")\n",
    "print(relevant_docs)\n",
    "\n",
    "# Create query pipeline \n",
    "# create index of concatenated content\n",
    "# search index for query\n",
    "# return relevant content\n",
    "# Indexing documents\n",
    "# Load the embedding model\n",
    "def load_embedding_model(model_name=\"sentence-transformers/all-mpnet-base-v2\", device=\"cuda\"):\n",
    "    model_kwargs = {\"device\": device}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    return HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "\n",
    "lc_embedding_model = load_embedding_model()\n",
    "embed_model = LangchainEmbedding(lc_embedding_model)\n",
    "Settings.embed_model = embed_model\n",
    "query = \"Explain what vendor service does\"\n",
    "\n",
    "index = VectorStoreIndex.from_documents(relevant_docs)\n",
    "\n",
    "# Setting up LLM and querying capabilities\n",
    "llm = Ollama(model=\"mistral\", request_timeout=60.0)\n",
    "Settings.llm = llm\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)\n",
    "\n",
    "# Template for queries\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information above, I want you to think step by step to answer the query in a crisp manner, in case you don't know the answer say 'I don't know!'.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "\n",
    "# Querying the index\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
