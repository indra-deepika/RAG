{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/miniconda3/envs/rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Your max_length is set to 30, but your input_length is only 4. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 30, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
      "Your max_length is set to 30, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 30, but your input_length is only 12. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n",
      "Your max_length is set to 30, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 10. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
      "Your max_length is set to 30, but your input_length is only 17. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4402 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Your max_length is set to 30, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 30, but your input_length is only 12. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 30, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 30, but your input_length is only 4. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 30, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
      "Your max_length is set to 30, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 30, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
      "Your max_length is set to 30, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 30, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 30, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
      "Your max_length is set to 30, but your input_length is only 4. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 30, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n",
      "Your max_length is set to 30, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 30, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 30, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
      "Your max_length is set to 30, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 30, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 30, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Your max_length is set to 30, but your input_length is only 4. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 30, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
      "Your max_length is set to 30, but your input_length is only 12. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 30, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 30, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 30, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Your max_length is set to 30, but your input_length is only 4. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 30, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
      "Your max_length is set to 30, but your input_length is only 12. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 7. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=3)\n",
      "Your max_length is set to 30, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Your max_length is set to 30, but your input_length is only 5. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 30, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n",
      "Your max_length is set to 30, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "transcript_directory = \"/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/\"\n",
    "\n",
    "# Initialize the summarization pipeline with an explicitly specified model\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "def generate_summary(code_content):\n",
    "    # Define the maximum number of tokens\n",
    "    max_tokens = 1024\n",
    "\n",
    "    comments = re.findall(r'# (.+)', code_content)\n",
    "    comments_text = ' '.join(comments)\n",
    "\n",
    "    functions_with_returns = re.findall(r'def (\\w+)\\((.*?)\\).*?:\\s*return (.+)', code_content, re.DOTALL)\n",
    "    function_calls_and_returns = [f\"{name}({args}) returns {ret.strip()}\" for name, args, ret in functions_with_returns]\n",
    "\n",
    "    summary = \"\"\n",
    "    \n",
    "    # Check if the total tokens in the input exceed the max_tokens limit\n",
    "    total_tokens = len(comments_text.split()) + sum(len(call_return.split()) for call_return in function_calls_and_returns)\n",
    "    if total_tokens > max_tokens:\n",
    "        return \"Input exceeds maximum token limit, summarization skipped.\"\n",
    "\n",
    "    # Dynamic max_length adjustment based on the input length\n",
    "    def get_max_length(text):\n",
    "        return min(130, max(30, len(text.split()) // 2))\n",
    "\n",
    "    if comments_text:\n",
    "        try:\n",
    "            summary_comments = summarizer(comments_text, max_length=get_max_length(comments_text), min_length=30, do_sample=False)\n",
    "            summary += summary_comments[0]['summary_text']\n",
    "        except Exception as e:\n",
    "            summary += \" Error summarizing comments: \" + str(e)\n",
    "        \n",
    "    for call_return in function_calls_and_returns:\n",
    "        if len(call_return.split()) < max_tokens:\n",
    "            try:\n",
    "                summary_function = summarizer(call_return, max_length=get_max_length(call_return), min_length=30, do_sample=False)\n",
    "                summary += ' ' + summary_function[0]['summary_text']\n",
    "            except Exception as e:\n",
    "                summary += \" Error summarizing function returns: \" + str(e)\n",
    "\n",
    "    return summary if summary else \"No data to summarize.\"\n",
    "\n",
    "# Lambda function for metadata with error handling\n",
    "filename_fn = lambda filename: {\n",
    "    'file_path': os.path.join(transcript_directory, filename),\n",
    "    'code_summary': generate_summary(open(os.path.join(transcript_directory, filename), 'r', errors='replace').read())\n",
    "}\n",
    "\n",
    "# Load documents with updated lambda function\n",
    "documents = SimpleDirectoryReader(transcript_directory, filename_as_id=True, file_metadata=filename_fn, recursive=True).load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# find documents with unique file paths\n",
    "unique_file_paths = set([doc.metadata['file_path'] for doc in documents])\n",
    "print(len(unique_file_paths))\n",
    "# print(unique_file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/vendor-service/run.py\n",
      " run.py from flask_migrate import Migrate migrate = Migrate(app, db) migrate migrate = migrate . migrate .  App. save_session(self, *args, **kwargs) returns jsonify({\"status\": \"OK\", 200)\n",
      "# run.py\n",
      "from application import create_app, db\n",
      "from application import models\n",
      "# from flask_migrate import Migrate\n",
      "\n",
      "app = create_app()\n",
      "# migrate = Migrate(app, db)\n",
      "from flask import jsonify\n",
      "\n",
      "from flask import g\n",
      "from flask.sessions import SecureCookieSessionInterface\n",
      "from flask_login import user_loaded_from_header\n",
      "\n",
      "\n",
      "class CustomSessionInterface(SecureCookieSessionInterface):\n",
      "    \"\"\"Prevent creating session from API requests.\"\"\"\n",
      "\n",
      "    def save_session(self, *args, **kwargs):\n",
      "        if g.get('login_via_header'):\n",
      "            return\n",
      "        return super(CustomSessionInterface, self).save_session(*args,\n",
      "                                                                **kwargs)\n",
      "\n",
      "\n",
      "app.session_interface = CustomSessionInterface()\n",
      "\n",
      "\n",
      "@user_loaded_from_header.connect\n",
      "def user_loaded_from_header(self, user=None):\n",
      "    g.login_via_header = True\n",
      "\n",
      "\n",
      "@app.route(\"/health\", methods=[\"GET\"])\n",
      "def health_check():\n",
      "    return jsonify({\"status\": \"OK\"}), 200\n",
      "if __name__ == '__main__':\n",
      "    app.run(host='0.0.0.0', port=5004)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# get metadata for the first document\n",
    "metadata = documents[108].metadata\n",
    "print(metadata['file_path'])\n",
    "print(metadata['code_summary'])\n",
    "\n",
    "\n",
    "print(documents[108].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "query = \"Explain what product service does\"\n",
    "# Initialize the question-answering pipeline with an explicitly specified model\n",
    "qa_pipeline = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant documents:\n",
      "[Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/frontend/application/frontend/api/ProductClient.py', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/frontend/application/frontend/api/ProductClient.py', 'code_summary': ' application/frontend/api/ProductClient.py product = response.json() product = product . Product is a product of a'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# application/frontend/api/ProductClient.py\\nimport requests\\n\\n\\nclass ProductClient:\\n\\n    @staticmethod\\n    def get_products(vendor_id=None, min_price=None, max_price=None):\\n        params = {}\\n        \\n        if vendor_id is not None:\\n            params[\\'vendor_id\\'] = vendor_id\\n            \\n        if min_price is not None:\\n            params[\\'min_price\\'] = min_price\\n            \\n        if max_price is not None:\\n            params[\\'max_price\\'] = max_price\\n\\n        r = requests.get(\\'http://cproduct-service:5002/api/products\\', params=params)\\n        products = r.json()\\n        return products\\n\\n    @staticmethod\\n    def get_product(slug):\\n        response = requests.request(method=\"GET\", url=\\'http://cproduct-service:5002/api/product/\\' + slug)\\n        product = response.json()\\n        return product\\n    \\n    @staticmethod\\n    def get_product_by_id(product_id):\\n        response = requests.request(method=\"GET\", url=\\'http://cproduct-service:5002/api/product/\\' + str(product_id))\\n        product = response.json()\\n        return product\\n    \\n    @staticmethod\\n    def create_product(form, vendor_id):\\n        payload = {\\n            \\'name\\': form.name.data,\\n            \\'slug\\': form.slug.data,\\n            \\'image\\': \"product1.jpg\",\\n            \\'price\\': form.price.data,\\n            \\'vendor_id\\': vendor_id\\n        }\\n        response = requests.request(\"POST\", url=\\'http://cproduct-service:5002/api/product/create\\', data=payload)\\n        # product = response.json()\\n        return response\\n    \\n    \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/order-service/application/models.py', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/order-service/application/models.py', 'code_summary': \" application/models.py is based on a database that looks at applications and models.py . Application/models .py is a database  create(self, user_id) returns { 'product': self.product_id, 'product' 'quantity': self\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"# application/models.py\\nfrom . import db\\nfrom datetime import datetime\\n\\nclass Order(db.Model):\\n    id = db.Column(db.Integer, primary_key=True)\\n    user_id = db.Column(db.Integer)\\n    items = db.relationship('OrderItem', backref='orderItem')\\n    is_open = db.Column(db.Boolean, default=True)\\n    date_added = db.Column(db.DateTime, default=datetime.utcnow)\\n    date_updated = db.Column(db.DateTime, onupdate=datetime.utcnow)\\n\\n    def create(self, user_id):\\n        self.user_id = user_id\\n        self.is_open = True\\n        return self\\n\\n    def to_json(self):\\n        items = []\\n        for i in self.items:\\n            items.append(i.to_json())\\n\\n        return {\\n            'items': items,\\n            'is_open': self.is_open,\\n            'user_id': self.user_id\\n        }\\n\\n\\nclass OrderItem(db.Model):\\n    id = db.Column(db.Integer, primary_key=True)\\n    order_id = db.Column(db.Integer, db.ForeignKey('order.id'))\\n    product_id = db.Column(db.Integer)\\n    quantity = db.Column(db.Integer, default=1)\\n    date_added = db.Column(db.DateTime, default=datetime.utcnow)\\n    date_updated = db.Column(db.DateTime, onupdate=datetime.utcnow)\\n\\n    def __init__(self, product_id, quantity):\\n        self.product_id = product_id\\n        self.quantity = quantity\\n\\n    def to_json(self):\\n        return {\\n            'product': self.product_id,\\n            'quantity': self.quantity\\n        }\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/product-service/application/product_api/__init__.py', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/product-service/application/product_api/__init__.py', 'code_summary': ' application/product_api/__init__.py is created by application application application . Application is created from application application to application application'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"# application/product_api/__init__.py\\nfrom flask import Blueprint\\n\\nproduct_api_blueprint = Blueprint('product_api', __name__)\\n\\nfrom . import routes\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_0', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"\\n\\nMastering Microservices with Python, Flask, and Docker\\nInterested in microservices, and how they can be used for increased agility and scalability?\\n\\nMicroservices is an architectural style and pattern that structures an application as a collection of coherent services. Each service is highly maintainable, testable, loosely coupled, independently deployable, and precisely focused.\\n\\nThis course takes a hands-on look at microservices using Python, Flask, and Docker. You'll learn how Flask can be used to quickly prototype and build microservices, as well as how to use Docker to host and deploy them.\\n\\n:metal:\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_8', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 5.\\nPopulate the product database:\\n```\\ncurl -i -d \"name=prod1&slug=prod1&image=product1.jpg&price=100\" -X POST localhost:5002/api/product/create\\ncurl -i -d \"name=prod2&slug=prod2&image=product2.jpg&price=200\" -X POST localhost:5002/api/product/create\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_9', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 6.\\nUsing your workstations browser - navigate to the following URL and register:\\n```\\nhttp://localhost:5000/register\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_10', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 7.\\nBack within your terminal, use a mysql client to confirm that a new user registration record was created:\\n```\\nmysql --host=127.0.0.1 --port=32000 --user=cloudacademy --password=pfm_2020\\nmysql> show databases;\\nmysql> use user;\\nmysql> show tables;\\nmysql> select * from user;\\nmysql> exit\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_11', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 8.\\nUsing your workstations browser - login, and add products into your cart, and then finally click the checkout option\\n```\\nhttp://localhost:5000/login\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_12', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 9.\\nBack within your terminal, use a mysql client to confirm that a new order has been created:\\n```\\nmysql --host=127.0.0.1 --port=32002 --user=cloudacademy --password=pfm_2020\\nmysql> show databases;\\nmysql> use order;\\nmysql> show tables;\\nmysql> select * from order.order;\\nmysql> select * from order.order_item;\\nmysql> exit\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_13', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nMicroservices Teardown\\nPerform the following steps to teardown the microservices environment:\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_1', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nProject Structure\\nThe Python Flask based microservices project is composed of the following 4 projects: \\n* frontend\\n* user-service\\n* product-service\\n* order-service\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_2', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nMicroservices Setup and Configuration\\nTo launch the end-to-end microservices application perform the following:\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_3', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 1.\\nNavigate into the frontend directory, and confirm the presence of the ```docker-compose.deploy.yml``` file:\\n```\\ncd frontend\\nls -la\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_4', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 1.\\nCreate a new Docker network and name it ```micro_network```:\\n```\\ndocker network create micro_network\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_5', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 2.\\nBuild each of the microservice Docker container images:\\n```\\ndocker-compose -f docker-compose.deploy.yml build\\ndocker images\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_6', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 3.\\nLaunch the microservice environment:\\n```\\ndocker-compose -f docker-compose.deploy.yml build\\ndocker ps -a\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_7', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 4.\\nPrepare each microservice mysql database:\\n```\\nfor service in corder-service cproduct-service cuser-service;\\ndo \\n docker exec -it $service flask db init\\n docker exec -it $service flask db migrate\\n docker exec -it $service flask db upgrade\\ndone\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_14', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 1.\\nCreate a new Docker network and name it ```micro_network```:\\n```\\nfor container in cuser-service cproduct-service corder-service cproduct_dbase cfrontend-app cuser_dbase corder_dbase;\\ndo\\n docker stop $container\\n docker rm $container\\ndone\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_15', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 2.\\nRemove the container volumes\\n```\\nfor vol in frontend_orderdb_vol frontend_productdb_vol frontend_userdb_vol;\\ndo\\n docker volume rm $vol\\ndone\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_16', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nStep 3.\\nRemove the container network\\n```\\ndocker network rm micro_network\\n```\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md_part_17', embedding=None, metadata={'file_path': '/mnt/c/Users/edeep/RAG/RAG_Codebase/project3_se-final_with_openai/project3_se-final_with_openai/README.md', 'code_summary': ' Mastering Microservices with Python, Flask, and Docker Project Structure . Project Structure: Microservices setup and Configuration . Teardown:'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\n\\nPython extensions reference\\nThe following Python extensions were used:\\n\\n* Flask-SQLAlchemy: https://flask-sqlalchemy.palletsprojects.com/en/2.x/\\n* Flask-Login: https://flask-login.readthedocs.io/en/latest/\\n* Flask-Migrate: https://github.com/miguelgrinberg/flask-migrate/\\n* Requests: https://requests.readthedocs.io/en/master/\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "import torch    \n",
    "from llama_index.core import VectorStoreIndex,  Settings, PromptTemplate\n",
    "from llama_index.llms.ollama import Ollama  # Assuming you corrected the import here\n",
    "\n",
    "# Retieve documents relevant to query based on meta data by filtering the documents\n",
    "# vector for query and code summar or file path should be close\n",
    "# to each other\n",
    "def find_relevant_documents(documents, query):\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    document_embeddings = embedder.encode([doc.metadata['code_summary'] for doc in documents], convert_to_tensor=True)\n",
    "    # scores = util.pytorch_cos_sim(query_embedding, document_embeddings)[0]\n",
    "    # # scores = [(doc.metadata['code_summary'], util.pytorch_cos_sim(query_embedding, doc['embedding'])[0][0].item()) for doc in documents]\n",
    "    # scores.sort(key=lambda x: x[1], reverse=True)  # Sort by score\n",
    "    similarities = util.pytorch_cos_sim(query_embedding, document_embeddings)[0]\n",
    "\n",
    "    # Get sorted scores and their indices in descending order\n",
    "    sorted_scores, sorted_indices = torch.sort(similarities, descending=True)\n",
    "    \n",
    "    # Filter to retain only documents with a score greater than 0.5\n",
    "    filtered_indices = sorted_indices[sorted_scores > 0.3]\n",
    "\n",
    "    # Retrieve the corresponding documents\n",
    "    relevant_docs = [documents[idx] for idx in filtered_indices]\n",
    "\n",
    "    return relevant_docs\n",
    "\n",
    "    # return [doc for doc, score in scores if score > 0.5]  # \n",
    "\n",
    "# now answer query based on actual content in the relevant documents\n",
    "# append doc contents \n",
    "relevant_docs = find_relevant_documents(documents, query)\n",
    "print(\"Relevant documents:\")\n",
    "print(relevant_docs)\n",
    "\n",
    "print(len(relevant_docs))\n",
    "\n",
    "# Concatenate contents of relevant documents\n",
    "concatenated_content = \" \".join(doc.text for doc in relevant_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " product service is one of the microservices in the given Python Flask based project. Its primary function is to handle all the product-related operations such as creating, retrieving, updating, and deleting products. Users can interact with this service through API endpoints provided by it. In the context information above, you can see that the product database is populated using the product-service API endpoints.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, PromptTemplate\n",
    "from llama_index.llms.ollama import Ollama  \n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # Correct class from HuggingFace\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core import Settings, VectorStoreIndex, PromptTemplate, SimpleDirectoryReader\n",
    "from llama_index.llms.ollama import Ollama  # Assuming you corrected the import here\n",
    "\n",
    "# print(\"Answer: \", answer['answer'], \"Score: \", answer['score'])\n",
    "\n",
    "# Create query pipeline \n",
    "# create index of concatenated content\n",
    "# search index for query\n",
    "# return relevant content\n",
    "# Indexing documents\n",
    "# Load the embedding model\n",
    "def load_embedding_model(model_name=\"sentence-transformers/all-mpnet-base-v2\", device=\"cuda\"):\n",
    "    model_kwargs = {\"device\": device}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    return HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "\n",
    "lc_embedding_model = load_embedding_model()\n",
    "embed_model = LangchainEmbedding(lc_embedding_model)\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = VectorStoreIndex.from_documents(relevant_docs)\n",
    "\n",
    "# Setting up LLM and querying capabilities\n",
    "llm = Ollama(model=\"mistral\", request_timeout=60.0)\n",
    "Settings.llm = llm\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)\n",
    "\n",
    "# Template for queries\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information above, I want you to think step by step to answer the query in a crisp manner, in case you don't know the answer say 'I don't know!'.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "\n",
    "query = \"Explain what product service does\"\n",
    "# Querying the index\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' product service is one of the microservices in the given Python Flask based '\n",
      " 'project. Its primary function is to handle all the product-related '\n",
      " 'operations such as creating, retrieving, updating, and deleting products. '\n",
      " 'Users can interact with this service through API endpoints provided by it. '\n",
      " 'In the context information above, you can see that the product database is '\n",
      " 'populated using the product-service API endpoints.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Use the PrettyPrinter to print the data\n",
    "pp.pprint(response.response_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
